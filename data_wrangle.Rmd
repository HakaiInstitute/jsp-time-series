---
title: "data wrangle for Hakai Juvenile Salmon Program Time Series"
author: "Brett Johnson"
date: "October 16, 2018"
output: html_document
---

This script is meant to pre-process the data required for publishing the Hakai Institute Juvenile Salmon Program Migration Observations Time Series. Most of the data that are read in this file are from the `hakaisalmon` R package which contains all the program data. The `hakaisalmon` r package is versioned so that any changes to data are kept track of. The `hakaisalmon` r package is a private repository, mainly because a number of graduate students are currently depending on this data to publish their theses, and out of respect for their projects the data will be requestable, not publically accesible, until students publish their research. Access can be requested in the meantime however and we will likely grant access. Please see the package website [here](https://hakaiinstitute.github.io/hakaisalmon/). A DOI citable version of the data in the R package can be formall requested [here](http://dx.doi.org/10.21966/1.566666).

A shiny app of these same data can be found [here](https://hecate.hakai.org/shiny/JSP/)


```{r setup}
library(hakaisalmon) #v0.2.1
library(tidyverse)
library(lubridate)
library(knitr)
library(here)
library(car)

f <- function(x) {
  format(x + as.Date("2018-01-01") -1, format = "%B %d")
}

# This same data warngling file should be used for the pdf report as well as the shiny app

survey_seines <- hakaisalmon::survey_seines %>% 
  # remove ad-hoc collections from migration timing calcs
  filter(survey_type == "standard",
         collection_protocol == "SEMSP",
         set_type == "targeted") %>%
  # only include consistently sampled sites with similar catchabilities
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  ) %>%
  mutate(year = year(survey_date),
         yday = yday(survey_date)
  ) %>% 
  # filter out sampling events from beyond the normal sampling period. (May 1 - July 9)
  filter(yday < 190) 

saveRDS(survey_seines, here::here("data", "survey_seines.RDS"))
write_csv(survey_seines, here::here("data", "survey_seines.csv"))

current_year <- max(year(survey_data$survey_date))
project_years <- current_year - 2015 + 1
study_range <- paste(2015, "-", current_year)

```

```{r sealice}
sealice_lab_motiles <- hakaisalmon::sealice_lab_motiles
sealice_field <- hakaisalmon::sealice_field

lab_lice <-
  left_join(survey_seines, hakaisalmon::fish_field_data) %>%
  left_join(sealice_lab_motiles) %>%
  drop_na(cm_lab) %>%
  # lump all sexes and staged of adult caligus
  mutate(
    motile_caligus_lab = rowSums(
      select_(., "cm_lab", "cpaf_lab", "caf_lab", "cgf_lab", "ucal_lab"),
      na.rm = T
    ),
    # lump all sexes and staged of adult leps
    motile_lep_lab = rowSums(
      select_(
        .,
        "lpam_lab",
        "lpaf_lab",
        "lam_lab",
        "laf_lab",
        "lgf_lab",
        "ulep_lab"
      ),
      na.rm = T
    )
  ) %>%
  select(
    ufn,
    site_id,
    semsp_id,
    species,
    region,
    survey_date,
    motile_caligus_lab,
    motile_lep_lab
  )

field_lice <-
  left_join(survey_seines, hakaisalmon::fish_field_data) %>%
  left_join(sealice_field) %>%
  # here I drop NAs from cm_field so that they wont be converted to zeros when rowSums na.rm = T coerces them to zero
  # the NAs will be created when I subsequently left_join
  drop_na(cm_field) %>%
  mutate(
    motile_caligus_field = rowSums(select_(
      ., "cm_field", "cpaf_field", "caf_field", "cgf_field"
    )),
    motile_lep_field = rowSums(
      select(
        .,
        "lpam_field",
        "lpaf_field",
        "lam_field",
        "laf_field",
        "lgf_field"
      ),
      na.rm = T
    )
  ) %>%
  select(
    ufn,
    site_id,
    semsp_id,
    species,
    region,
    survey_date,
    motile_caligus_field,
    motile_lep_field
  )

motile_lice <- full_join(lab_lice, field_lice)

sealice_time_series <- motile_lice %>%
  # with preference to lab ID, combine field and lab ID columns into one column
  mutate(motile_caligus = coalesce(motile_caligus_lab, motile_caligus_field)) %>%
  mutate(motile_lep = coalesce(motile_lep_lab, motile_lep_field)) %>%
  mutate(all_lice = motile_caligus + motile_lep) %>%
  select(ufn,
         survey_date,
         site_id,
         region,
         species,
         motile_lep,
         motile_caligus,
         all_lice) %>%
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  ) %>%
  gather(`motile_caligus`,
         `motile_lep`,
         `all_lice`,
         key = louse_species,
         value = n_lice) %>%
  drop_na() %>%
  mutate(year = year(survey_date)
  ) %>% 
  filter(species %in% c("SO", "PI", "CU"))

write_csv(sealice_time_series, here("data", "sealice_time_series.csv"))
saveRDS(sealice_time_series, here("data", "sealice_time_series.rds"))

abundance_df <- sealice_time_series %>% 
  group_by(year, region, louse_species, species) %>% 
  summarize(abundance =  mean(n_lice, na.rm = T), 
            sd = sd(n_lice, na.rm = T),
            n = n()) %>% 
  mutate(se = sd / sqrt(n),
         abundance_ci = qt(1 - (0.05 / 2), n - 1) * se) %>% 
  ungroup() %>% 
  mutate_if(is.numeric, round, 2) %>% 
  rename(Abundance = abundance) %>%  
  # mutate("Abundance, 95% CI" = paste(Abundance, "+/-", lower_ci)) %>% 
  select(year, region, species, louse_species, n, Abundance, abundance_ci)

prevalence <- sealice_time_series %>%
  mutate(lousy = ifelse(n_lice > 0, "has_lice", "no_lice")) %>% 
  group_by(year, region, louse_species, species, lousy) %>% 
  summarize(n = n()) %>% 
  spread(key = lousy, value = n) %>% 
  mutate(n = sum(has_lice, no_lice), prevalence = has_lice / n) %>% 
  drop_na() %>% 
  do(broom::tidy(binom.test(.$has_lice, .$n, 0.5, alternative = "two.sided",
                            conf.level = 0.95))) %>% 
  rename("prevalence" = "estimate") %>% 
  ungroup() %>% 
  mutate_if(is.numeric, round, 2) %>% 
  rename(Prevalence = prevalence, n = parameter, prevalence_ci = conf.low) %>% 
  select(year, region, species, louse_species, n, Prevalence, prevalence_ci)

intensity <- sealice_time_series %>%
  filter(n_lice > 0) %>%
  select(year, region,  louse_species, species, n_lice) %>%
  group_by(year, region, species, louse_species) %>%
  summarise(intensity =  mean(n_lice, na.rm = T), 
            sd = sd(n_lice, na.rm = T),
            n = n()) %>% 
  mutate(se = sd / sqrt(n),
         intensity_ci = qt(1 - (0.05 / 2), n - 1) * se) %>% 
  mutate_if(is.numeric, round, 2) %>% 
  rename(Intensity = intensity) %>% 
  select(year, region, species, louse_species, Intensity, intensity_ci)

sealice_summary_table <- full_join(abundance_df, prevalence) %>% 
  full_join(intensity) %>% 
  rename(Year = year, Region = region, Species = species) %>% 
  mutate(Year = as.character(Year))

write.csv(sealice_summary_table, here("data", "sealice_summary_table.csv"))
saveRDS(sealice_summary_table, here("data", "sealice_summary_table.RDS"))

```

```{r sealice bootstrapping}
library(rsample)
library(dplyr)
library(tidyr)
library(broom)
library(purrr)

# I think this is it, but I can't figure out how to iterate over all the environments
library(modelr)
mean_n_lice <- function(splits) {
  x <- analysis(splits)
  mean(x$n_lice)
}

# this_is_it correctly nests the data into its hierarchical form
this_is_it <- sealice_time_series %>% 
  filter(louse_species != "all_lice") %>% 
  split(list(.$year, .$region, .$site_id, .$survey_date, .$species, .$louse_species))

# remove empty permutations
row_lt1 <- which(sapply(this_is_it, nrow) < 1)
this_is_it <- this_is_it[-row_lt1]

# Written by Biljana
boots <- lapply(1:length(this_is_it), function(x){bootstraps(this_is_it[[x]], times=1000)})
 
boot_samples <- lapply(1:length(boots),function(x){  
  boots[[x]] %>% 
  mutate(samples = unlist(map(splits, mean_n_lice)))})


extract.boot.CI <- lapply(1:length(boot_samples), function(x){quantile(boot_samples[[x]]$samples,c(0.025, 0.975))} )

extract.boot.CI=do.call(rbind,extract.boot.CI)
# Back to Brett's code
extract.boot.CI <- as_tibble(extract.boot.CI) 
extract.boot.CI$estimate <- (extract.boot.CI$`97.5%` + extract.boot.CI$`2.5%`) / 2
category <- names(this_is_it)
extract.boot.CI$category <- category

lice_bootstraps <- extract.boot.CI %>% 
separate(category, into = c("year", "region", "site_id", "year2", "month", "day", "species", "stage", "louse_species")) %>% 
  select(-year, year = year2)

readr::write_csv(lice_bootstraps, here("data", "lice_bootstraps.csv"))
saveRDS(lice_bootstraps, here("data", "lice_bootstraps.rds"))

beepr::beep(8)
```

```{r Catch Intensity}
# In 2015 and 2016 we only enumerated seines with sockeye. In 2017 onwards we enumerated catch of all seines even if they didn't have sockeye. This created a problem because from those first two years of the program we only have pink and chum abundance measurements when they were caught with sockeye. So the only inter-annual comparable abundance parameter across years is 'intensity'â€” the number of sockeye, pink, and chum that were caught when greater than zero sockeye were caught.

catch_intensity <- survey_seines %>% 
  rename("Sockeye" = "so_total", "Pink" = "pi_total", "Chum" = "cu_total") %>%
  # remove seines that did not catch any sockeye, even if they caught other spp
  filter(Sockeye > 0) %>% 
  select(year, Sockeye, Pink, Chum) %>% 
  gather(key = species, value = catch, - year) %>% 
  # Filter out catches where no pink and chum were caught, so that catch intensity is consistent for each species. Ie. catch intensity for sockeye is the average number of sockeye when catch of sockeye is > 0, catch intensity of pink is the average number of pink caught when > 1 pink was caught, and so on for chum as well.
  filter(catch > 0)

catch_intensity <- catch_intensity %>% 
  group_by(year, species) %>% 
  summarize(mean_catch = mean(catch),
            sd = sd(catch),
            n = n())%>% 
  mutate(se = sd / sqrt(n),
         lower_ci = qt(1 - (0.05 / 2), n - 1) * se,
         upper_ci = qt(1 - (0.05 / 2), n - 1) * se) %>% 
  ungroup() %>% 
  mutate_if(is.numeric, round, 1)

write_csv(catch_intensity, here::here("data", "catch_intensity.csv"))
saveRDS(catch_intensity, here::here("data", "catch_intensity.RDS"))
```

```{r Migration Timing}
# I first take Cumulative abundance

tidy_catch <- survey_seines %>%
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total
  ) %>%
  # filter out instances when sockeye were not caught because in 2015 we only enumerated sets that had sockeye in them, and moving forward we plan on enumerating all sets. If we included sets that had zero sockeye in 2017 and 2018 then our CPUE of sockeye in 2015 and 2016 would be biased high. Therefore these abundance metrics are strictly based on seines which had sockeye and we are then measuring how many of each species are in a seine, from seines with sockeye, as our metric of all species abundance and proportion. This is the only way to resolve inconsistent sampling strategies between years.
  filter(so_total > 0) %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

tidy_catch <- as.data.frame(tidy_catch)

tidy_catch <- tidy_catch %>%
  mutate(yday = yday(survey_date))

# TIME SERIES AVERAGES

# Create a table with time series average daily proportion for each species and each region for all years combined (2015-2018)

#Sockeye
so_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "so_total") %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

so_total_catch_expanded_cum_DI <-
  so_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

so_total_predict_average_prop_DI <-
  log_cumul_abund(
    so_total_catch_expanded_cum_DI$percent,
    so_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "so_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

#Pink
pi_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "pi_total") %>%
  # figure out if a year is odd or even and filter out odd years
  mutate(even_or_odd = year %% 2) %>% 
  filter(even_or_odd != 1) %>% 
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

pi_total_catch_expanded_cum_DI <-
  pi_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

pi_total_predict_average_prop_DI <-
  log_cumul_abund(
    pi_total_catch_expanded_cum_DI$percent,
    pi_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "pi_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Chum
cu_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "cu_total") %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

cu_total_catch_expanded_cum_DI <-
  cu_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

cu_total_predict_average_prop_DI <-
  log_cumul_abund(
    cu_total_catch_expanded_cum_DI$percent,
    cu_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "cu_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create table of time series average migration timing for DI
predict_average_prop <-
  rbind(
    pi_total_predict_average_prop_DI,
    so_total_predict_average_prop_DI,
    cu_total_predict_average_prop_DI
  ) %>%
  drop_na(daily_percent)

predict_average_prop$species <- factor(predict_average_prop$species) %>% 
  fct_recode("Sockeye" = "so_total", "Pink" = "pi_total", "Chum" = "cu_total")

saveRDS(predict_average_prop,
        here::here("data", "predict_average_prop.RDS"))
write_csv(predict_average_prop,
          here::here("data", "predict_average_prop.csv"))

# ANNUAL CUMULATIVE ABUNDANCE CURVES FOR EACH SPECIES

# Discovery Islands only, not doing this for Johnstone Strait
daily_mean_cumul_abund_annual <- tidy_catch %>%
  ungroup() %>%
  filter(region == "DI") %>%
  mutate(year = as.factor(year)) %>%
  group_by(year, region, species) %>%
  arrange(year, region, species, yday) %>% 
  mutate(cumsum = cumsum(n),
          percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    survey_date = yday,
    region = region,
    species = species,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  group_by(year, region, species, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

#2015 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2015 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2015)

so_predict_annual_prop_DI_2015 <-
  log_cumul_abund(so_cum_abund_annual_DI_2015$percent,
                  so_cum_abund_annual_DI_2015$survey_date) %>%
  mutate(region = "DI", year = 2015) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# chum
cu_cum_abund_annual_DI_2015 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2015)

cu_predict_annual_prop_DI_2015 <-
  log_cumul_abund(cu_cum_abund_annual_DI_2015$percent,
                  cu_cum_abund_annual_DI_2015$survey_date) %>%
  mutate(region = "DI", year = 2015) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2016 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2016)

so_predict_annual_prop_DI_2016 <-
  log_cumul_abund(so_cum_abund_annual_DI_2016$percent,
                  so_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2016)

pi_predict_annual_prop_DI_2016 <-
  log_cumul_abund(pi_cum_abund_annual_DI_2016$percent,
                  pi_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2016 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2016)

cu_predict_annual_prop_DI_2016 <-
  log_cumul_abund(cu_cum_abund_annual_DI_2016$percent,
                  cu_cum_abund_annual_DI_2016$survey_date) %>%
  mutate(region = "DI", year = 2016) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

#2017 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2017 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2017)

so_predict_annual_prop_DI_2017 <-
  log_cumul_abund(so_cum_abund_annual_DI_2017$percent,
                  so_cum_abund_annual_DI_2017$survey_date) %>%
  mutate(region = "DI", year = 2017) %>%
  mutate(daily_percent = y - lag(y), species = "SO")


## chum
cu_cum_abund_annual_DI_2017 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2017)
          
cu_predict_annual_prop_DI_2017 <-
  log_cumul_abund(cu_cum_abund_annual_DI_2017$percent,
                  cu_cum_abund_annual_DI_2017$survey_date) %>%
  mutate(region = "DI", year = 2017) %>%
  mutate(daily_percent = y - lag(y), species = "CU")

<<<<<<< HEAD
cu_cum_abund_annual_DI_2017$species <- "Chum"

# For now, I'll fjust use raw points without a line
write_csv(cu_cum_abund_annual_DI_2017, here("data", "cu_cum_abund_DI_2017.csv"))
          
# cu_predict_annual_prop_DI_2017 <-
#   log_cumul_abund(cu_cum_abund_annual_DI_2017$percent,
#                   cu_cum_abund_annual_DI_2017$survey_date) %>%
#   mutate(region = "DI", year = 2017) %>%
#   mutate(daily_percent = y - lag(y), species = "CU")
=======
# Getting singular gradient warning for 2017 chum
cu_cum_abund_2017 <- cu_cum_abund_annual_DI_2017 %>% 
  ungroup() %>% 
  mutate(species = "Chum")

# For now, I'll fjust use raw points without a line
write_csv(cu_cum_abund_2017, here("data", "cu_cum_abund_2017.csv"))
>>>>>>> fe1d968e0056764ac50b130232d9a1bf4b13b7d3

#2018 modeled cumulative abundance
# sockeye
so_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "so_total", year == 2018)

so_predict_annual_prop_DI_2018 <-
  log_cumul_abund(so_cum_abund_annual_DI_2018$percent,
                  so_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "SO")

# pink
pi_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "pi_total", year == 2018)

pi_predict_annual_prop_DI_2018 <-
  log_cumul_abund(pi_cum_abund_annual_DI_2018$percent,
                  pi_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "PI")

# chum
cu_cum_abund_annual_DI_2018 <- daily_mean_cumul_abund_annual %>%
  filter(species == "cu_total", year == 2018)

cu_predict_annual_prop_DI_2018 <-
  log_cumul_abund(cu_cum_abund_annual_DI_2018$percent,
                  cu_cum_abund_annual_DI_2018$survey_date) %>%
  mutate(region = "DI", year = 2018) %>%
  mutate(daily_percent = y - lag(y), species = "CU")


# Put each years modelled cumulative abundnance into one table
predict_annual_prop <-
  rbind(
    so_predict_annual_prop_DI_2015,
    cu_predict_annual_prop_DI_2015,
    so_predict_annual_prop_DI_2016,
    pi_predict_annual_prop_DI_2016,
    cu_predict_annual_prop_DI_2016,
    so_predict_annual_prop_DI_2017,
    #cu_predict_annual_prop_DI_2017, Won't include this year b/c it doesn't seem like the log_cumul_abund function works for these data.
    so_predict_annual_prop_DI_2018,
    pi_predict_annual_prop_DI_2018,
    cu_predict_annual_prop_DI_2018
  )

predict_annual_prop$species <- factor(predict_annual_prop$species) %>% 
  fct_recode("Sockeye" = "SO", "Pink" = "PI", "Chum" = "CU")

write_csv(predict_annual_prop, here::here("data", "predict_annual_prop.csv"))
saveRDS(predict_annual_prop, here::here("data", "predict_annual_prop.RDS"))

# Create data table for real (not modelled), q1, q2, q3 migration timing stats
peak_dates <- tidy_catch[rep(row.names(tidy_catch), tidy_catch$n), 1:6] %>%
  filter(species %in% c("so_total", "pi_total", "cu_total")) %>%
  mutate(yday = yday(survey_date)) %>%
  group_by(year, region, species) %>%
  summarise(n = n(), q1 = quantile(yday, probs = 0.25), q3 = quantile(yday, probs = 0.75), median = median(yday)) %>%
  ungroup() %>% 
  mutate(species = replace(species, species == "so_total", "SO")) %>%
  mutate(species = replace(species, species == "pi_total", "PI")) %>%
  mutate(species = replace(species, species == "cu_total", "CU")) %>% 
  mutate(Spread = median - q1) %>% 
  mutate(q1 = ifelse(species == "PI" & year %% 2 == 1, NA, q1),
         median = ifelse(species == "PI" & year %% 2 == 1, NA, median),
         q3 = ifelse(species == "PI" & year %% 2 == 1, NA, q3),
         Spread = ifelse(species == "PI" & year %% 2 == 1, NA, Spread)) %>% 
    mutate(year = as.character(year))

tsa_peak_dates <- peak_dates %>%
  ungroup() %>% 
  group_by(region, species) %>% 
  summarise(n = mean(n, na.rm = TRUE),
            q1 = mean(q1, na.rm = TRUE),
            median = mean(median, na.rm = TRUE),
            q3 = mean(q3, na.rm = TRUE),
            Spread = mean(Spread, na.rm = TRUE)
            ) %>% 
  mutate(year = paste(2015, "-", current_year)) %>% 
  select(year, everything())

peak_dates <- bind_rows(tsa_peak_dates, peak_dates)

write_csv(peak_dates, here("data","peak_dates.csv"))
saveRDS(peak_dates, here("data", "peak_dates.RDS"))


```

```{r Condition}

library('hakaiApi')
library('tidyverse')
library(lubridate)

# Run this line independently before the rest of the code to get the API authentication
client <- hakaiApi::Client$new() # Follow stdout prompts to get an API token

# Make a data request for chlorophyll data
endpoint <- sprintf("%s/%s", client$api_root, "eims/views/output/jsp_fish?limit=-1")
fish_cond <- client$get(endpoint) %>% 
  select(date, species, weight, fork_length) %>% 
  filter(species %in% c('SO', 'PI', 'CU', 'CO')) %>% 
  mutate(year = factor(year(date)))

write_csv(fish_cond, here("data", "fish_cond.csv"))


```

```{r Species Proportions}
# SPECIES PROPORTIONS

spp_prop <- survey_seines %>%
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total
  ) %>%
  # Next I remove instances when no sockeye were caught. I'm doing this because in 2015 and 2016 we only enumerated catches in which we caught sockeye, whereas in 2017 and 2018 we enumerated all seines. So to reduce the bias introduced from the field method i filter the comparison down to what is comparable
  filter(so_total > 0) %>%
  # remove instances when not all species were enumerated by droping rows with NA
  drop_na() %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

spp_prop_expanded <-
  spp_prop[rep(row.names(spp_prop), spp_prop$n), 1:6] %>%
  mutate(yday = yday(survey_date), year = year(survey_date))

proportions <- spp_prop_expanded %>%
  group_by(year, species) %>%
  summarize(n = n()) %>%
  mutate(proportion = n / sum(n)) %>%
  ungroup()

saveRDS(proportions, here::here("data", "proportion.RDS"))
write_csv(proportions, here::here("data", "proportion.csv"))
```

```{r SST}
# Get CTD data from EIMS database using R API

# Run this line indpendently, check console for URL, and authorize
client <- hakaiApi::Client$new()

qu39_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=QU39&limit=-1")

qu39_all <- client$get(qu39_endpoint) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

qu29_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=QU29&limit=-1")
qu29_all <- client$get(qu29_endpoint) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

js2_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=JS2&limit=-1")
js2_all <- client$get(js2_endpoint)  %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

js12_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=JS12&limit=-1")
js12_all <- client$get(js12_endpoint)  %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

js2_12_all <- rbind(js2_all, js12_all)

js2_12_all$station <- "js2_12"

## Create time series of average conditions which includes the current year, using a loess function

ctd_all <- rbind(qu39_all, qu29_all, js2_12_all) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt),
    week = week(start_dt)
  ) %>%
  filter(depth <= 30) %>%
  select(
    year,
    date,
    week,
    yday,
    station,
    conductivity,
    temperature,
    depth,
    salinity,
    dissolved_oxygen_ml_l
  ) %>%
  group_by(station, date, yday) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = T),
    mean_do = mean(dissolved_oxygen_ml_l, na.rm = T),
    mean_salinity = mean(salinity, na.rm = T)
  )

saveRDS(ctd_all, here::here("data", "ctd_all.RDS"))
write_csv(ctd_all, here::here("data", "ctd_all.csv"))

## Create current year data to compare to time series
ctd_post_time_series <- rbind(qu39_all, qu29_all, js2_12_all) %>%
  filter(year == 2018, yday > 32, yday < 213,  depth <= 30) %>%
  select(
    year,
    date,
    yday,
    station,
    conductivity,
    temperature,
    depth,
    salinity,
    dissolved_oxygen_ml_l
  ) %>%
  group_by(station, yday) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = T),
    mean_do = mean(dissolved_oxygen_ml_l, na.rm = T),
    mean_salinity = mean(salinity, na.rm = T)
  )


## SST ANOMALY DATA
## QU39
qu39_average <- ctd_all %>%
  filter(station == "QU39")

# Filter down to station of interest
qu39_this_year <- ctd_post_time_series %>%
  filter(station == "QU39")

temp.lo_qu39 <-
  loess(mean_temp ~ yday, qu39_average, SE = T, span = 0.65)

#create table for predicitions from loess function
sim_temp_data_qu39 <-
  tibble(yday = seq(min(qu39_average$yday), max(qu39_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_qu39$predicted_mean_temp <-
  predict(temp.lo_qu39, sim_temp_data_qu39, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line so that colour of plot will change above and below the trend line.
# This code will have to be re-written manually to interpolate the mid points of when current year points cross over the trendline
# See https://stackoverflow.com/questions/27135962/how-to-fill-geom-polygon-with-different-colors-above-and-below-y-0
qu39_temp_anomaly_data <-
  left_join(sim_temp_data_qu39, qu39_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff) %>%
  add_row(
    station = "QU39",
    yday = 47.5,
    predicted_mean_temp = predict(temp.lo_qu39, 47.5),
    mean_temp = predict(temp.lo_qu39, 47.5)
  ) %>%
  add_row(
    station = "QU39",
    yday = 124,
    predicted_mean_temp = predict(temp.lo_qu39, 124),
    mean_temp = predict(temp.lo_qu39, 124)
  ) %>%
  add_row(
    station = "QU39",
    yday = (145 + 149) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (145 + 149) / 2),
    mean_temp = predict(temp.lo_qu39, (145 + 149) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = (155 + 149) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (155 + 149) / 2),
    mean_temp = predict(temp.lo_qu39, (155 + 149) / 2)
  ) %>%
  add_row(station = "QU39",
    yday = (162 + 169) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (162 + 169) / 2),
    mean_temp = predict(temp.lo_qu39, (162 + 169) / 2)) %>% 
  add_row(station = "QU39",
    yday = (171 + 169) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (171 + 169) / 2),
    mean_temp = predict(temp.lo_qu39, (171 + 169) / 2)) %>% 
  add_row(
    station = "QU39",
    yday = (192 + 177) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (192 + 177) / 2),
    mean_temp = predict(temp.lo_qu39, (192 + 177) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = 211.5,
    predicted_mean_temp = predict(temp.lo_qu39, 211.5),
    mean_temp = predict(temp.lo_qu39, 211.5)
  )

# Create min and max for any given day of the time series
qu39_min_max <- qu39_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "QU39")

##QU29
qu29_average <- ctd_all %>%
  filter(station == "QU29")

# Filter down to station of interest
qu29_this_year <- ctd_post_time_series %>%
  filter(station == "QU29")

temp.lo_qu29 <- loess(mean_temp ~ yday, qu29_average, span = 0.65)

#create table for predicitions from loess function
sim_temp_data_qu29 <-
  tibble(yday = seq(min(qu29_average$yday), max(qu29_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_qu29$predicted_mean_temp <-
  predict(temp.lo_qu29, sim_temp_data_qu29, SE = T)

# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line
qu29_temp_anomaly_data <-
  left_join(sim_temp_data_qu29, qu29_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff) %>%
  add_row(
    station = "QU29",
    yday = (51 + 36) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (51 + 36) / 2),
    mean_temp = predict(temp.lo_qu29, (51 + 36) / 2)
  ) %>%
  add_row(
    station = "QU29",
    yday = (157 + 136) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (157 + 136) / 2),
    mean_temp = predict(temp.lo_qu29, (157 + 136) / 2)
  ) %>%
  add_row(
    station = "QU29",
    yday = (157 + 201) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (157 + 201) / 2),
    mean_temp = predict(temp.lo_qu29, (157 + 201) / 2)
  )
# Create min and max for any given day of the time series

qu29_min_max <- qu29_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "QU29")

# JS2+12
js2_12_average <- ctd_all %>%
  filter(station == "js2_12")

# Filter down to station of interest
js2_12_this_year <- ctd_post_time_series %>%
  filter(station == "js2_12")

temp.lo_js2_12 <-
  loess(mean_temp ~ yday,
        js2_12_average,
        SE = T,
        span = 0.65)

#create table for predicitions from loess function
sim_temp_data_js2_12 <-
  tibble(yday = seq(min(js2_12_average$yday), max(js2_12_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_js2_12$predicted_mean_temp <-
  predict(temp.lo_js2_12, sim_temp_data_js2_12, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line
js2_12_temp_anomaly_data <-
  left_join(sim_temp_data_js2_12, js2_12_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff)

# Create min and max for any given day of the time series
js2_12_min_max <- js2_12_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "js2_12")


min_max_data <- rbind(js2_12_min_max, qu39_min_max, qu29_min_max)
saveRDS(min_max_data,  here::here("data", "min_max_temps.RDS"))
write_csv(min_max_data,  here::here("data", "min_max_temps.csv"))

average_temps <- rbind(qu39_average, qu29_average, js2_12_average)
saveRDS(average_temps, here::here("data", "average_temps.RDS"))
write_csv(average_temps, here::here("data", "average_temps.csv"))

temperature_anomaly_data <-
  rbind(js2_12_temp_anomaly_data,
        qu29_temp_anomaly_data,
        qu39_temp_anomaly_data)
saveRDS(temperature_anomaly_data,
        here::here("data", "temperature_anomaly_data.RDS"))
write_csv(temperature_anomaly_data,
          here::here("data", "temperature_anomaly_data.csv"))
```

```{r Heatmap}
# HEATMAP Create heatmaps for key parameters

# Migration Timing z-scores

# Sockeye migrations
so_peak_dates <- peak_dates %>% 
  filter(species == "SO", region == "DI", year != study_range) 

# TODO: check for normality or use non parametric value
so_mean_of_median_date <- mean(so_peak_dates$median)
so_sd_of_median_date <- sd(so_peak_dates$median)

so_peak_dates <- so_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = (median - so_mean_of_median_date) / so_sd_of_median_date)

so_migration_dates_z <- so_peak_dates$z_score

# Pink migrations
PI_peak_dates <- peak_dates %>% 
  filter(species == "PI", region == "DI", year != study_range) 

PI_mean_of_median_date <- mean(PI_peak_dates$median, na.rm = TRUE)
PI_sd_of_median_date <- sd(PI_peak_dates$median, na.rm = TRUE)

 
PI_peak_dates <- PI_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = (median - PI_mean_of_median_date) / PI_sd_of_median_date)

PI_migration_dates_z <- PI_peak_dates$z_score

# Chum migrations
CU_peak_dates <- peak_dates %>% 
  filter(species == "CU", region == "DI", year != study_range) 

CU_mean_of_median_date <- mean(CU_peak_dates$median)
CU_sd_of_median_date <- sd(CU_peak_dates$median)

CU_peak_dates <- CU_peak_dates %>% 
  arrange(year) %>% 
  mutate(z_score = (median - CU_mean_of_median_date) / CU_sd_of_median_date)

CU_migration_dates_z <- CU_peak_dates$z_score


fish_data <- left_join(hakaisalmon::fish_field_data, hakaisalmon::fish_lab_data) %>%
  # combine both fork length measurements
  mutate(
    fork_length_lab = as.numeric(fork_length),
    fork_length_field = as.numeric(fork_length_field),
    fork_length = coalesce(fork_length_lab, fork_length_field)
  )

length_histo <- left_join(survey_seines, fish_data)  %>%
  select(survey_date, region, species, fork_length) %>%
  drop_na(fork_length) %>%
  mutate(year = year(survey_date)) %>%
  # Remove incidtentaly sampled species
  filter(species != "CK", species != "CO", species != "HE") %>%
  mutate(year = as.factor(year))

saveRDS(length_histo, here::here("data", "length_histo.RDS"))
write_csv(length_histo, here::here("data", "length_histo.csv"))

# Length z-scores

# Sockeye lengths
so_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "SO")

so_avg_length <- as.numeric(so_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

so_sd_length <- as.numeric(so_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

so_length_z <- (so_mean_sd$mean - so_avg_length[2]) / so_sd_length[2]

# Pink lengths
PI_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "PI")

PI_avg_length <- as.numeric(PI_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

PI_sd_length <- as.numeric(PI_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

PI_length_z <- (PI_mean_sd$mean - PI_avg_length[2]) / PI_sd_length[2]

# Chum lengths
CU_mean_sd <- length_histo %>% 
  group_by(year, species) %>% 
  summarise(mean = mean(fork_length, na.rm = T), sd = sd(fork_length)) %>% 
  filter(species == "CU")

CU_avg_length <- as.numeric(CU_mean_sd %>% 
                              group_by(species) %>% 
                              summarize(mean_fl = mean(mean, na.rm = T)))

CU_sd_length <- as.numeric(CU_mean_sd %>% 
                             group_by(species) %>% 
                             summarize(sd_fl = sd(mean, na.rm = T)))

CU_length_z <- (CU_mean_sd$mean - CU_avg_length[2]) / CU_sd_length[2]

# Sealice z-scores

# Sockeye sealice
so_api_annual_mean <- sealice_summary_table %>% 
  ungroup() %>% 
  filter(louse_species == "all_lice") %>% 
  filter(Species == "SO") %>% 
  select(Year, Species, Prevalence, Abundance, Intensity) %>% 
  group_by(Year) %>% 
  summarize_all(c("mean", "sd")) %>% 
  mutate(Species = "SO")

so_api_mean <- mean(so_api_annual_mean$Prevalence_mean)
so_api_sd <- sd(so_api_annual_mean$Prevalence_mean)

so_api_z <- (so_api_annual_mean$Prevalence_mean - so_api_mean) / so_api_sd

# Pink sealice
PI_api_annual_mean <- sealice_summary_table %>% 
  ungroup() %>% 
  filter(louse_species == "all_lice") %>% 
  filter(Species == "PI") %>% 
  select(Year, Species, Prevalence, Abundance, Intensity) %>% 
  group_by(Year) %>% 
  summarize_all(c("mean", "sd"), na.rm = T) 

PI_api_mean <- mean(PI_api_annual_mean$Prevalence_mean)
PI_api_sd <- sd(PI_api_annual_mean$Prevalence_mean)

PI_api_z <- (PI_api_annual_mean$Prevalence_mean - PI_api_mean) / PI_api_sd

# Chum sealice
CU_api_annual_mean <- sealice_summary_table %>% 
  ungroup() %>% 
  filter(louse_species == "all_lice") %>% 
  filter(Species == "CU") %>% 
  select(Year, Species, Prevalence, Abundance, Intensity) %>% 
  group_by(Year) %>% 
  summarize_all(c("mean", "sd"), na.rm = T) 

CU_api_mean <- mean(CU_api_annual_mean$Abundance_mean)
CU_api_sd <- sd(CU_api_annual_mean$Abundance_mean)

CU_api_z <- (CU_api_annual_mean$Abundance_mean - CU_api_mean) / CU_api_sd

# Calculate catch intensity z-scores

# Sockeye catch intensity
so_catch_intensity <- catch_intensity %>% 
  filter(species == "Sockeye")

so_mean_catch_intensity <- mean(so_catch_intensity$mean_catch)
so_sd_catch_intensity <- sd(so_catch_intensity$mean_catch)

so_catch_intensity <- so_catch_intensity %>% 
  mutate(z_score = (mean_catch - so_mean_catch_intensity) / so_sd_catch_intensity)

so_catch_intensity_z <- so_catch_intensity$z_score

# Pink catch intensity
pi_catch_intensity <- catch_intensity %>% 
  filter(species == "Pink")

pi_mean_catch_intensity <- mean(pi_catch_intensity$mean_catch)
pi_sd_catch_intensity <- sd(pi_catch_intensity$mean_catch)

pi_catch_intensity <- pi_catch_intensity %>% 
  mutate(z_score = (mean_catch - pi_mean_catch_intensity) / pi_sd_catch_intensity)

pi_catch_intensity_z <- pi_catch_intensity$z_score

# Chum catch intensity
cu_catch_intensity <- catch_intensity %>% 
  filter(species == "Chum")

cu_mean_catch_intensity <- mean(cu_catch_intensity$mean_catch)
cu_sd_catch_intensity <- sd(cu_catch_intensity$mean_catch)

cu_catch_intensity <- cu_catch_intensity %>% 
  mutate(z_score = (mean_catch - cu_mean_catch_intensity) / cu_sd_catch_intensity)

cu_catch_intensity_z <- cu_catch_intensity$z_score

# SST z-score

# I'm going to filter the time period down to May and June because that's the most relevant period that juvenile salmon are likely to be in the Strait of Georgia

sst_annual_mean <- ctd_all %>%
  ungroup() %>% 
  mutate(date = as_date(date)) %>% 
  mutate(month = month(date), year = year(date)) %>% 
  filter(month >= 5 & month <= 6, station == "QU39") %>% 
  select(year, mean_temp) %>% 
  group_by(year) %>% 
  summarise(sd_temp = sd(mean_temp, na.rm = TRUE),
            mean_temp = mean(mean_temp, na.rm = TRUE))

sst_mean <- mean(sst_annual_mean$mean_temp)  
sst_sd <- sd(sst_annual_mean$mean_temp)

sst_z <- (sst_annual_mean$mean_temp - sst_mean) / sst_sd

sst_anomaly_table <- tibble(year = c(2015:current_year),
                            Estimate = round(sst_annual_mean$mean_temp, 2),
                            SD = round(sst_annual_mean$sd_temp, 2),
                            Z = sst_z)

# Put all the z-scores together into a table


mean <- c(f(so_peak_dates$median), so_catch_intensity$mean_catch, round(so_mean_sd$mean, 1), so_api_annual_mean$Abundance_mean,
          f(PI_peak_dates$median), pi_catch_intensity$mean_catch, round(PI_mean_sd$mean, 1), PI_api_annual_mean$Abundance_mean,
          f(CU_peak_dates$median), cu_catch_intensity$mean_catch, round(CU_mean_sd$mean, 1), CU_api_annual_mean$Abundance_mean)

SD <- c(rep(NA, project_years), so_catch_intensity$sd, so_mean_sd$sd, so_api_annual_mean$Abundance_sd,
        rep(NA, project_years), pi_catch_intensity$sd, PI_mean_sd$sd, PI_api_annual_mean$Abundance_sd,
        rep(NA, project_years), cu_catch_intensity$sd, CU_mean_sd$sd, CU_api_annual_mean$Abundance_sd)

ts_mean <- rep(c(f(so_mean_of_median_date), so_mean_catch_intensity, round(so_avg_length[2], 1), so_api_mean,
                 f(PI_mean_of_median_date), pi_mean_catch_intensity, round(PI_avg_length[2], 1), PI_api_mean,
                 f(CU_mean_of_median_date), cu_mean_catch_intensity, round(CU_avg_length[2], CU_api_mean), 1), each= project_years)

Z <- c(so_migration_dates_z, so_catch_intensity_z, so_length_z, so_api_z, 
       PI_migration_dates_z, pi_catch_intensity_z, PI_length_z, PI_api_z, 
       CU_migration_dates_z, cu_catch_intensity_z, CU_length_z, CU_api_z) 

n_species <- 3 # sockeye, pink, chum


#
heatmap_data <- tibble(year = rep(c(2015:current_year), n_species * project_years),
                       Estimate = mean, SD = SD, Z = Z)
                      
heatmap_data <- rbind(heatmap_data, sst_anomaly_table) %>% 
 mutate(Z = round(Z, 2),
        SD = round(SD, 2)) 

vec <- rep(rep(c("Migration Timing", "Catch Intensity", "Length", "Parasite Loads"), each = project_years), n_species)
vec2 <- rep(c("Sea-surface Temperature"), project_years)
vec <- c(vec, vec2)
heatmap_data$measure <- vec

spp_vec <- rep(c("Sockeye", "Pink", "Chum"), each = 16)
vec3 <- rep(c("Northern Strait of Georgia"), project_years)
spp_vec <- c(spp_vec, vec3)

heatmap_data$spp <- spp_vec


write_csv(heatmap_data, here("data", "heatmap_data.csv"))
saveRDS(heatmap_data, here("data", "heatmap_data.RDS"))

beepr::beep(13)
```

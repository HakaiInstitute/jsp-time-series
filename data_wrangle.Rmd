---
title: "data wrangle"
author: "Brett Johnson"
date: "October 16, 2018"
output: html_document
---

This script is meant to pre-process the data required for publishing the Hakai Institute Juvenile Salmon Program Time Series. Most of the data the data that are read in this file are from the `hakaisalmon` R package which contains all the program data. The `hakaisalmon` r package is versioned so that any changes to data are kept track of. The `hakaisalmon` r package is a private repository, mainly because a number of graduate students are currently depending on this data to publish their theses, and out of respect for their projects the data will be requestable, not publically accesible, until students publish their research. Access can be requested in the meantime however and we will likely grant access. Please see the package website [here](https://hakaiinstitute.github.io/hakaisalmon/). A DOI citable version of the data in the R package can be formall requested [here](http://dx.doi.org/10.21966/1.566666).

A shiny app of these same data can be found [here](https://hecate.hakai.org/shiny/JSP/)

```{r setup}
library(hakaisalmon) #v0.2.1
library(tidyverse)
library(lubridate)
library(knitr)
library(here)
library(car)

# This same data warngling file should be used for the pdf report as well as the shiny app

survey_seines <- hakaisalmon::survey_seines %>% 
   # remove ad-hoc collections from migration timing calcs
  filter(survey_type == "standard",
         collection_protocol == "SEMSP",
         set_type == "targeted") %>%
  # only include consistently sampled sites with similar catchabilities
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  ) %>%
  mutate(year = year(survey_date),
         yday = yday(survey_date)
         ) %>% 
  # filter out sampling events from beyond the normal sampling period. (May 1 - July 9)
  filter(yday < 190) 

saveRDS(survey_seines, here::here("data", "survey_seines.RDS"))
write_csv(survey_seines, here::here("data", "survey_seines.csv"))

```

```{r sealice}
sealice_lab_motiles <- hakaisalmon::sealice_lab_motiles
sealice_field <- hakaisalmon::sealice_field

lab_lice <-
  left_join(survey_seines, hakaisalmon::fish_field_data) %>%
  left_join(sealice_lab_motiles) %>%
  drop_na(cm_lab) %>%
  # lump all sexes and staged of adult caligus
  mutate(
    motile_caligus_lab = rowSums(
      select_(., "cm_lab", "cpaf_lab", "caf_lab", "cgf_lab", "ucal_lab"),
      na.rm = T
    ),
    # lump all sexes and staged of adult leps
    motile_lep_lab = rowSums(
      select_(
        .,
        "lpam_lab",
        "lpaf_lab",
        "lam_lab",
        "laf_lab",
        "lgf_lab",
        "ulep_lab"
      ),
      na.rm = T
    )
  ) %>%
  select(
    ufn,
    site_id,
    semsp_id,
    species,
    region,
    survey_date,
    motile_caligus_lab,
    motile_lep_lab
  )

field_lice <-
  left_join(survey_seines, hakaisalmon::fish_field_data) %>%
  left_join(sealice_field) %>%
  # here I drop NAs from cm_field so that they wont be converted to zeros when rowSums na.rm = T coerces them to zero
  # the NAs will be created when I subsequently left_join
  drop_na(cm_field) %>%
  mutate(
    motile_caligus_field = rowSums(select_(
      ., "cm_field", "cpaf_field", "caf_field", "cgf_field"
    )),
    motile_lep_field = rowSums(
      select(
        .,
        "lpam_field",
        "lpaf_field",
        "lam_field",
        "laf_field",
        "lgf_field"
      ),
      na.rm = T
    )
  ) %>%
  select(
    ufn,
    site_id,
    semsp_id,
    species,
    region,
    survey_date,
    motile_caligus_field,
    motile_lep_field
  )

motile_lice <- full_join(lab_lice, field_lice)

sealice_time_series <- motile_lice %>%
  # with preference to lab ID, combine field and lab ID columns into one column
  mutate(motile_caligus = coalesce(motile_caligus_lab, motile_caligus_field)) %>%
  mutate(motile_lep = coalesce(motile_lep_lab, motile_lep_field)) %>%
  mutate(all_lice = motile_caligus + motile_lep) %>%
  select(ufn,
         survey_date,
         site_id,
         region,
         species,
         motile_lep,
         motile_caligus,
         all_lice) %>%
  filter(
    site_id %in% c(
      "D07",
      "D09",
      "D22",
      "D27",
      "D10",
      "D08",
      "D34",
      "D20",
      "J03",
      "J02",
      "J09",
      "J11"
    )
  ) %>%
  gather(`motile_caligus`,
         `motile_lep`,
         `all_lice`,
         key = louse_species,
         value = n_lice) %>%
  drop_na() %>%
  mutate(year = year(survey_date)
  )

write.csv(sealice_time_series, here("data", "sealice_time_series.csv"))
saveRDS(sealice_time_series, here("data", "sealice_time_series.RDS"))


## Generate sealice prevalence column
motile_infected_hosts <- sealice_time_series %>%
  filter(n_lice > 0, species %in% c("SO", "PI", "CU")) %>%
  group_by(year, region, species, louse_species) %>%
  summarise(n_infected = n())

hosts <- sealice_time_series %>%
  group_by(year, region, species, louse_species) %>%
  summarise(n = n()) %>%
  filter(species %in% c("SO", "PI", "CU"))

summary_sealice <- left_join(hosts, motile_infected_hosts) %>%
  replace_na(list(n_infected = 0)) %>%
  mutate(prevalence = n_infected /  n) %>%
  select(-n_infected)

# Generate abundance column for summary_sealice
abundance <- sealice_time_series %>%
  select(year, region, species, louse_species, n_lice) %>%
  group_by(year, region, species, louse_species) %>%
  summarise(
    abundance = mean(n_lice, na.rm = T),
    n = n(),
    abundance_sd = sd(n_lice)
  )

summary_sealice <- left_join(summary_sealice, abundance)

# Generate intensity column
intensity <- sealice_time_series %>%
  filter(n_lice > 0) %>%
  select(year, region, species, louse_species, n_lice) %>%
  group_by(year, region, species, louse_species) %>%
  summarise(intensity =  mean(n_lice, na.rm = T))

summary_sealice <- left_join(summary_sealice, intensity)

saveRDS(summary_sealice, here::here("data", "summary_sealice.RDS"))
write_csv(summary_sealice, here::here("data", "summary_sealice.csv"))
```

```{r SST}
# Get CTD data from EIMS database using R API

# Run this line indpendently, check console for URL, and authorize
client <- hakaiApi::Client$new()

qu39_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=QU39&limit=-1")

qu39_all <- client$get(qu39_endpoint) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

qu29_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=QU29&limit=-1")
qu29_all <- client$get(qu29_endpoint) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

js2_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=JS2&limit=-1")
js2_all <- client$get(js2_endpoint)  %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

js12_endpoint <-
  sprintf("%s/%s",
          client$api_root,
          "ctd/views/file/cast/data?station=JS12&limit=-1")
js12_all <- client$get(js12_endpoint)  %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt)
  )

js2_12_all <- rbind(js2_all, js12_all)

js2_12_all$station <- "js2_12"

## Create time series of average conditions which includes the current year, using a loess function

ctd_all <- rbind(qu39_all, qu29_all, js2_12_all) %>%
  mutate(
    year = year(start_dt),
    date = as_date(start_dt),
    yday = yday(start_dt),
    week = week(start_dt)
  ) %>%
  filter(depth <= 30) %>%
  select(
    year,
    date,
    week,
    yday,
    station,
    conductivity,
    temperature,
    depth,
    salinity,
    dissolved_oxygen_ml_l
  ) %>%
  group_by(station, date, yday) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = T),
    mean_do = mean(dissolved_oxygen_ml_l, na.rm = T),
    mean_salinity = mean(salinity, na.rm = T)
  )

saveRDS(ctd_all, here::here("data", "ctd_all.RDS"))
write_csv(ctd_all, here::here("data", "ctd_all.csv"))

## Create current year data to compare to time series
ctd_post_time_series <- rbind(qu39_all, qu29_all, js2_12_all) %>%
  filter(year == 2018, yday > 32, yday < 213,  depth <= 30) %>%
  select(
    year,
    date,
    yday,
    station,
    conductivity,
    temperature,
    depth,
    salinity,
    dissolved_oxygen_ml_l
  ) %>%
  group_by(station, yday) %>%
  summarise(
    mean_temp = mean(temperature, na.rm = T),
    mean_do = mean(dissolved_oxygen_ml_l, na.rm = T),
    mean_salinity = mean(salinity, na.rm = T)
  )


## SST ANOMALY DATA
## QU39
qu39_average <- ctd_all %>%
  filter(station == "QU39")

# Filter down to station of interest
qu39_this_year <- ctd_post_time_series %>%
  filter(station == "QU39")

temp.lo_qu39 <-
  loess(mean_temp ~ yday, qu39_average, SE = T, span = 0.65)

#create table for predicitions from loess function
sim_temp_data_qu39 <-
  tibble(yday = seq(min(qu39_average$yday), max(qu39_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_qu39$predicted_mean_temp <-
  predict(temp.lo_qu39, sim_temp_data_qu39, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line so that colour of plot will change above and below the trend line.
# This code will have to be re-written manually to interpolate the mid points of when current year points cross over the trendline
# See https://stackoverflow.com/questions/27135962/how-to-fill-geom-polygon-with-different-colors-above-and-below-y-0
qu39_temp_anomaly_data <-
  left_join(sim_temp_data_qu39, qu39_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff) %>%
  add_row(
    station = "QU39",
    yday = 47.5,
    predicted_mean_temp = predict(temp.lo_qu39, 47.5),
    mean_temp = predict(temp.lo_qu39, 47.5)
  ) %>%
  add_row(
    station = "QU39",
    yday = 124,
    predicted_mean_temp = predict(temp.lo_qu39, 124),
    mean_temp = predict(temp.lo_qu39, 124)
  ) %>%
  add_row(
    station = "QU39",
    yday = (145 + 149) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (145 + 149) / 2),
    mean_temp = predict(temp.lo_qu39, (145 + 149) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = (155 + 149) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (155 + 149) / 2),
    mean_temp = predict(temp.lo_qu39, (155 + 149) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = (192 + 177) / 2,
    predicted_mean_temp = predict(temp.lo_qu39, (192 + 177) / 2),
    mean_temp = predict(temp.lo_qu39, (192 + 177) / 2)
  ) %>%
  add_row(
    station = "QU39",
    yday = 211.5,
    predicted_mean_temp = predict(temp.lo_qu39, 211.5),
    mean_temp = predict(temp.lo_qu39, 211.5)
  )

# Create min and max for any given day of the time series
qu39_min_max <- qu39_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "QU39")

##QU29
qu29_average <- ctd_all %>%
  filter(station == "QU29")

# Filter down to station of interest
qu29_this_year <- ctd_post_time_series %>%
  filter(station == "QU29")

temp.lo_qu29 <- loess(mean_temp ~ yday, qu29_average, span = 0.65)

#create table for predicitions from loess function
sim_temp_data_qu29 <-
  tibble(yday = seq(min(qu29_average$yday), max(qu29_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_qu29$predicted_mean_temp <-
  predict(temp.lo_qu29, sim_temp_data_qu29, SE = T)

# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line
qu29_temp_anomaly_data <-
  left_join(sim_temp_data_qu29, qu29_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff) %>%
  add_row(
    station = "QU29",
    yday = (51 + 36) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (51 + 36) / 2),
    mean_temp = predict(temp.lo_qu29, (51 + 36) / 2)
  ) %>%
  add_row(
    station = "QU29",
    yday = (157 + 136) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (157 + 136) / 2),
    mean_temp = predict(temp.lo_qu29, (157 + 136) / 2)
  ) %>%
  add_row(
    station = "QU29",
    yday = (157 + 201) / 2,
    predicted_mean_temp = predict(temp.lo_qu29, (157 + 201) / 2),
    mean_temp = predict(temp.lo_qu29, (157 + 201) / 2)
  )
# Create min and max for any given day of the time series

qu29_min_max <- qu29_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "QU29")

# JS2+12
js2_12_average <- ctd_all %>%
  filter(station == "js2_12")

# Filter down to station of interest
js2_12_this_year <- ctd_post_time_series %>%
  filter(station == "js2_12")

temp.lo_js2_12 <-
  loess(mean_temp ~ yday,
        js2_12_average,
        SE = T,
        span = 0.65)

#create table for predicitions from loess function
sim_temp_data_js2_12 <-
  tibble(yday = seq(min(js2_12_average$yday), max(js2_12_average$yday), 0.1))
#Predict temp in 0.1 day increments to provide smooth points to join
sim_temp_data_js2_12$predicted_mean_temp <-
  predict(temp.lo_js2_12, sim_temp_data_js2_12, SE = T)


# Create a linear interpolation of points that have zero difference between
# loess model and 'observed data' so that an area plot will look right
# manually identify intersections and create values that fall on the line
js2_12_temp_anomaly_data <-
  left_join(sim_temp_data_js2_12, js2_12_this_year) %>%
  mutate(diff = if_else(mean_temp > predicted_mean_temp, "pos", "neg")) %>%
  drop_na(diff)

# Create min and max for any given day of the time series
js2_12_min_max <- js2_12_all %>%
  filter(depth <= 30) %>%
  group_by(year, yday) %>%
  summarise(mean_temp = mean(temperature)) %>%
  ungroup() %>%
  group_by(yday) %>%
  summarise(min_temp = min(mean_temp),
            max_temp = max(mean_temp)) %>%
  mutate(station = "js2_12")


min_max_data <- rbind(js2_12_min_max, qu39_min_max, qu29_min_max)
saveRDS(min_max_data,  here::here("data", "min_max_temps.RDS"))
write_csv(min_max_data,  here::here("data", "min_max_temps.csv"))

average_temps <- rbind(qu39_average, qu29_average, js2_12_average)
saveRDS(average_temps, here::here("data", "average_temps.RDS"))
write_csv(average_temps, here::here("data", "average_temps.csv"))

temperature_anomaly_data <-
  rbind(js2_12_temp_anomaly_data,
        qu29_temp_anomaly_data,
        qu39_temp_anomaly_data)
saveRDS(temperature_anomaly_data,
        here::here("data", "temperature_anomaly_data.RDS"))
write_csv(temperature_anomaly_data,
        here::here("data", "temperature_anomaly_data.csv"))
```

```{r Catch Intensity}
# In 2015 and 2016 we only enumerated seines with sockeye. In 2017 onwards we enumerated catch of all seines even if they didn't have sockeye. This created a problem because from those first two years of the program we only have pink and chum abundance measurements when they were caught with sockeye. So the only inter-annual comparable abundance parameter across years is 'intensity'â€” the number of sockeye, pink, and chum that were caught when greater than zero sockeye were caught.

catch_intensity <- survey_seines %>% 
  rename("Sockeye" = "so_total", "Pink" = "pi_total", "Chum" = "cu_total") %>%
  # remove seines that did not catch any sockeye, even if they caught other spp
  filter(Sockeye > 0) %>% 
  select(year, Sockeye, Pink, Chum) %>% 
  gather(key = species, value = catch, - year) %>% 
  # Filter out catches where no pink and chum were caught, so that catch intensity is consistent for each species. Ie. catch intensity for sockeye is the average number of sockeye when catch of sockeye is > 0, catch intensity of pink is the average number of pink caught when > 1 pink was caught, and so on for chum as well.
  filter(catch > 0)

catch_intensity <- catch_intensity %>% 
  group_by(year, species) %>% 
  summarize(mean_catch = mean(catch),
            sd = sd(catch),
            n = n())%>% 
  mutate(se = sd / sqrt(n),
         lower_ci = qt(1 - (0.05 / 2), n - 1) * se,
         upper_ci = qt(1 - (0.05 / 2), n - 1) * se) %>% 
  ungroup() %>% 
  mutate_if(is.numeric, round)

write_csv(catch_intensity, here::here("data", "catch_intensity.csv"))
saveRDS(catch_intensity, here::here("data", "catch_intensity.RDS"))
```

```{r Migration Timing}
# Cumulative abundance

tidy_catch <- survey_seines %>%
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total
  ) %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

tidy_catch <- as.data.frame(tidy_catch)

tidy_catch <- tidy_catch %>%
  mutate(yday = yday(survey_date)) %>%
  # remove pinks from non pink years and constrain the sample to before July 9th
  filter(!(species == "pi_total" &
             year %in% c(2015, 2017)), yday < 190)

saveRDS(tidy_catch, here::here("data", "tidy_catch.RDS"))
write_csv(tidy_catch, here::here("data", 'tidy_catch.csv'))

# Create a table with time series average daily proportion for each species and each region for all years combined (2015-2018)
#Sockeye
so_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "so_total") %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

# Create Discovery Islands time series of cumulative abundance migration timing
so_total_catch_expanded_cum_DI <-
  so_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

so_total_predict_average_prop_DI <-
  log_cumul_abund(
    so_total_catch_expanded_cum_DI$percent,
    so_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "so_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create Johnstone Strait time series of cumulative abundance migration timing
so_total_catch_expanded_cum_JS <-
  so_total_daily_mean_cumul_abund %>%
  filter(region == "JS")

so_total_predict_average_prop_JS <-
  log_cumul_abund(
    so_total_catch_expanded_cum_JS$percent,
    so_total_catch_expanded_cum_JS$survey_date
  ) %>%
  mutate(region = "JS", species = "so_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

#Pink
pi_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "pi_total") %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

# Create Discovery Islands time series of cumulative abundance migration timing
pi_total_catch_expanded_cum_DI <-
  pi_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

pi_total_predict_average_prop_DI <-
  log_cumul_abund(
    pi_total_catch_expanded_cum_DI$percent,
    pi_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "pi_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create Johnstone Strait time series of cumulative abundance migration timing
pi_total_catch_expanded_cum_JS <-
  pi_total_daily_mean_cumul_abund %>%
  filter(region == "JS")

pi_total_predict_average_prop_JS <-
  log_cumul_abund(
    pi_total_catch_expanded_cum_JS$percent,
    pi_total_catch_expanded_cum_JS$survey_date
  ) %>%
  mutate(region = "JS", species = "pi_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Chum
cu_total_daily_mean_cumul_abund <- tidy_catch %>%
  #First create cumulative proportion for each year individually
  filter(species == "cu_total") %>%
  mutate(
    yday = yday(survey_date),
    week = week(survey_date),
    year = year(survey_date)
  ) %>%
  group_by(year, region) %>%
  mutate(percent = cumsum(n / sum(n) * 100),
         non_cumul_prop = n / sum(n)) %>%
  ungroup() %>%
  transmute(
    year = year,
    week = week,
    survey_date = yday,
    region = region,
    percent = percent,
    non_cumul_prop = non_cumul_prop
  ) %>%
  ungroup() %>%
  # Second; average the daily prop for each region for all years combined
  group_by(region, survey_date) %>%
  summarize(
    percent = mean(percent),
    non_cumul_prop = mean(non_cumul_prop) * 100,
    n = n()
  )

# Create Discovery Islands time series of cumulative abundance migration timing
cu_total_catch_expanded_cum_DI <-
  cu_total_daily_mean_cumul_abund %>%
  filter(region == "DI")

cu_total_predict_average_prop_DI <-
  log_cumul_abund(
    cu_total_catch_expanded_cum_DI$percent,
    cu_total_catch_expanded_cum_DI$survey_date
  ) %>%
  mutate(region = "DI", species = "cu_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create Johnstone Strait time series of cumulative abundance migration timing
cu_total_catch_expanded_cum_JS <-
  cu_total_daily_mean_cumul_abund %>%
  filter(region == "JS")

cu_total_predict_average_prop_JS <-
  log_cumul_abund(
    cu_total_catch_expanded_cum_JS$percent,
    cu_total_catch_expanded_cum_JS$survey_date
  ) %>%
  mutate(region = "JS", species = "cu_total") %>%
  mutate(year = "2015-2018") %>%
  mutate(daily_percent = y - lag(y))

# Create table of time series average migration timing for JS and DI separate
predict_average_prop <-
  rbind(
    pi_total_predict_average_prop_DI,
    pi_total_predict_average_prop_JS,
    so_total_predict_average_prop_DI,
    so_total_predict_average_prop_JS,
    cu_total_predict_average_prop_DI,
    cu_total_predict_average_prop_JS
  ) %>%
  drop_na(daily_percent)

saveRDS(predict_average_prop,
        here::here("data", "predict_average_prop.RDS"))
write_csv(predict_average_prop,
        here::here("data", "predict_average_prop.csv"))
```


```{r Species Proportions}
# SPECIES PROPORTIONS

spp_prop <- survey_seines %>%
  select(
    survey_date,
    seine_id,
    region,
    so_total,
    pi_total,
    cu_total,
    co_total,
    he_total
  ) %>%
  # Next I remove instances when no sockeye were caught. I'm doing this because in 2015 and 2016 we only enumerated catches in which we caught sockeye, whereas in 2017 and 2018 we enumerated all seines. So to reduce the bias introduced from the field method i filter the comparison down to what is comparable
  filter(so_total > 0) %>%
  # remove instances when not all species were enumerated by droping rows with NA
  drop_na() %>%
  mutate(year = year(survey_date)) %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  ) %>%
  drop_na()

spp_prop_expanded <-
  spp_prop[rep(row.names(spp_prop), spp_prop$n), 1:6] %>%
  mutate(yday = yday(survey_date), year = year(survey_date))

proportions <- spp_prop_expanded %>%
  group_by(year, species) %>%
  summarize(n = n()) %>%
  mutate(proportion = n / sum(n)) %>%
  ungroup()

saveRDS(proportions, here::here("data", "proportion.RDS"))
write_csv(proportions, here::here("data", "proportion.csv"))
```

```{r Heatmap}
# HEATMAP Create heatmaps for key parameters

# Calculate migration date z-scores
tidy_catch <- survey_seines %>%
  select(survey_date,
         seine_id,
         region,
         so_total,
         pi_total,
         cu_total,
         co_total,
         he_total) %>%
  # filter out instances when sockeye were not caught because in 2015 we only enumerated sets that had sockeye in them, and moving forward we plan on enumerating all sets. If we included sets that had zero sockeye in 2017 and 2018 then our CPUE of sockeye in 2015 and 2016 would be biased high. Therefore this abundance metrics are strictly based on seines which had sockeye and we are then measuring how many of each species are in a seine, from seines with sockeye, as our metric of all species abundance and proportion. This is the only way to resolved inconsistent sampling strategies between years.
  filter(so_total > 0) %>%
  mutate(year = year(survey_date)) %>%
  # remove instances when there was not a complete enumeration of all species in the seine
  drop_na() %>%
  gather(
    `so_total`,
    `pi_total`,
    `cu_total`,
    `co_total`,
    `he_total`,
    key = "species",
    value = "n"
  )

peak_dates <-
  tidy_catch[rep(row.names(tidy_catch), tidy_catch$n), 1:6] %>%
  filter(species %in% c("so_total", "pi_total", "cu_total"),
         region == "DI") %>%
  mutate(yday = yday(survey_date)) %>%
  group_by(year, region, species) %>%
  summarise(n = n(), q1 = quantile(yday, probs = 0.25), q3 = quantile(yday, probs = 0.75), median = median(yday)) %>%
  mutate(species = replace(species, species == "so_total", "SO")) %>%
  mutate(species = replace(species, species == "pi_total", "PI")) %>%
  mutate(species = replace(species, species == "cu_total", "CU"))

saveRDS(peak_dates, here::here("data", "peak_dates.RDS"))
write_csv(peak_dates, here::here("data", "peak_dates.csv"))

# Length z-scores

fish_data <- left_join(hakaisalmon::fish_field_data, hakaisalmon::fish_lab_data) %>%
  # combine both fork length measurements
  mutate(
    fork_length_lab = as.numeric(fork_length),
    fork_length_field = as.numeric(fork_length_field),
    fork_length = coalesce(fork_length_lab, fork_length_field)
  )

length_histo <- left_join(survey_seines, fish_data)  %>%
  select(survey_date, region, species, fork_length) %>%
  drop_na(fork_length) %>%
  mutate(year = year(survey_date)) %>%
  # Remove incidtentaly sampled species
  filter(species != "CK", species != "CO", species != "HE") %>%
  mutate(year = as.factor(year))

saveRDS(length_histo, here::here("data", "length_histo.RDS"))
write_csv(length_histo, here::here("data", "length_histo.csv"))
```